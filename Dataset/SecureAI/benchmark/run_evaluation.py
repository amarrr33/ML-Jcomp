#!/usr/bin/env python3
"""
SecureAI Benchmark Evaluation System
Evaluates detection performance on CyberSecEval3 dataset with augmentation
"""

import sys
import os
import json
import csv
import time
from datetime import datetime
from pathlib import Path
import pandas as pd
import numpy as np

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from agents import TextGuardianAgent, SecureAICrew
from utils.llama_client import get_llama_client

print("="*80)
print("üéØ SECUREAI BENCHMARK EVALUATION SYSTEM")
print("="*80)


class BenchmarkEvaluator:
    """
    Comprehensive benchmark evaluation for SecureAI
    """
    
    def __init__(self, output_dir='./benchmark_results'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        print("\n[1/5] Initializing components...")
        self.guardian = TextGuardianAgent()
        self.llama = get_llama_client()
        
        self.results = []
        self.start_time = None
        
        print("‚úÖ Evaluator initialized")
    
    def load_dataset(self, dataset_path='../data/cyberseceval3-visual-prompt-injection-expanded.csv'):
        """Load CyberSecEval3 dataset"""
        print(f"\n[2/5] Loading dataset from {dataset_path}...")
        
        try:
            df = pd.read_csv(dataset_path)
            print(f"‚úÖ Loaded {len(df)} examples")
            
            # Show distribution
            if 'language' in df.columns:
                print(f"\nLanguage distribution:")
                for lang, count in df['language'].value_counts().items():
                    print(f"  {lang}: {count} examples")
            
            return df
        except Exception as e:
            print(f"‚ùå Error loading dataset: {e}")
            return None
    
    def augment_dataset(self, df, num_synthetic=100):
        """
        Augment dataset with synthetic examples using Llama
        """
        print(f"\n[3/5] Augmenting dataset with {num_synthetic} synthetic examples...")
        
        synthetic_data = []
        attack_types = [
            'instruction_injection',
            'prompt_leak',
            'role_manipulation',
            'jailbreak',
            'context_manipulation'
        ]
        
        examples_per_type = num_synthetic // len(attack_types)
        
        for attack_type in attack_types:
            print(f"  Generating {examples_per_type} {attack_type} examples...")
            
            try:
                # Generate adversarial examples
                prompt = f"""Generate {examples_per_type} examples of {attack_type} attacks on AI systems.
Each example should be a single line of text that attempts this attack.
Format: one attack per line, no numbering."""
                
                response = self.llama.generate(prompt, max_tokens=500)
                examples = [line.strip() for line in response.split('\n') 
                           if line.strip() and not line.strip().startswith(('1.', '2.', '3.'))]
                
                for example in examples[:examples_per_type]:
                    synthetic_data.append({
                        'text': example,
                        'language': 'en',
                        'is_adversarial': 1,
                        'attack_type': attack_type,
                        'source': 'llama_synthetic'
                    })
            
            except Exception as e:
                print(f"    ‚ö†Ô∏è Generation failed: {e}")
        
        # Generate benign examples
        benign_count = num_synthetic // 5
        print(f"  Generating {benign_count} benign examples...")
        
        try:
            prompt = f"""Generate {benign_count} normal, benign user queries to an AI assistant.
These should be legitimate questions or requests with no malicious intent.
Format: one query per line, no numbering."""
            
            response = self.llama.generate(prompt, max_tokens=300)
            examples = [line.strip() for line in response.split('\n') 
                       if line.strip() and not line.strip().startswith(('1.', '2.', '3.'))]
            
            for example in examples[:benign_count]:
                synthetic_data.append({
                    'text': example,
                    'language': 'en',
                    'is_adversarial': 0,
                    'attack_type': 'none',
                    'source': 'llama_synthetic'
                })
        
        except Exception as e:
            print(f"    ‚ö†Ô∏è Generation failed: {e}")
        
        # Combine with original dataset
        synthetic_df = pd.DataFrame(synthetic_data)
        
        # Ensure ground truth column exists
        if 'is_adversarial' not in df.columns:
            # Infer from other columns if possible
            df['is_adversarial'] = 1  # Assume adversarial for CyberSecEval
        
        combined_df = pd.concat([df, synthetic_df], ignore_index=True)
        
        print(f"‚úÖ Dataset augmented: {len(df)} ‚Üí {len(combined_df)} examples")
        print(f"   Original: {len(df)} | Synthetic: {len(synthetic_data)}")
        
        return combined_df
    
    def evaluate_example(self, text, ground_truth, language='en'):
        """
        Evaluate a single example
        """
        start_time = time.time()
        
        try:
            # Run detection
            result = self.guardian.analyze(text, language)
            
            processing_time = time.time() - start_time
            
            # Determine blocking decision (threshold at 0.5)
            blocked = 1 if result['aggregate_score'] >= 0.5 else 0
            
            # Determine risk level
            score = result['aggregate_score']
            if score >= 0.8:
                risk_level = 'CRITICAL'
            elif score >= 0.6:
                risk_level = 'HIGH'
            elif score >= 0.4:
                risk_level = 'MEDIUM'
            elif score >= 0.2:
                risk_level = 'LOW'
            else:
                risk_level = 'SAFE'
            
            return {
                'text': text[:100],  # Truncate for CSV
                'language': language,
                'ground_truth': int(ground_truth),
                'detected': int(result['is_adversarial']),
                'blocked': blocked,
                'confidence': float(result['confidence']),
                'aggregate_score': float(result['aggregate_score']),
                'risk_level': risk_level,
                'processing_time_ms': round(processing_time * 1000, 2),
                'pattern_score': float(result.get('pattern_score', 0)),
                'entropy_score': float(result.get('entropy_score', 0.5)),
                'zero_shot_score': float(result.get('zero_shot_score', 0.5)),
                'topological_score': float(result.get('topological_score', 0.5))
            }
        
        except Exception as e:
            print(f"    ‚ö†Ô∏è Evaluation failed: {e}")
            return {
                'text': text[:100],
                'language': language,
                'ground_truth': int(ground_truth),
                'detected': 0,
                'blocked': 0,
                'confidence': 0.0,
                'aggregate_score': 0.0,
                'risk_level': 'ERROR',
                'processing_time_ms': 0,
                'pattern_score': 0,
                'entropy_score': 0,
                'zero_shot_score': 0,
                'topological_score': 0
            }
    
    def run_benchmark(self, df, sample_size=None):
        """
        Run benchmark on dataset
        """
        print(f"\n[4/5] Running benchmark evaluation...")
        
        # Sample if requested
        if sample_size and sample_size < len(df):
            df_sample = df.sample(n=sample_size, random_state=42)
            print(f"  Using sample of {sample_size} examples")
        else:
            df_sample = df
            print(f"  Evaluating all {len(df)} examples")
        
        self.start_time = time.time()
        self.results = []
        
        total = len(df_sample)
        for idx, row in enumerate(df_sample.iterrows(), 1):
            _, data = row
            
            if idx % 10 == 0 or idx == total:
                elapsed = time.time() - self.start_time
                rate = idx / elapsed if elapsed > 0 else 0
                print(f"  Progress: {idx}/{total} ({idx/total*100:.1f}%) | "
                      f"{rate:.1f} examples/sec | "
                      f"Elapsed: {elapsed:.1f}s")
            
            text = data.get('text', data.get('prompt', ''))
            ground_truth = data.get('is_adversarial', 1)
            language = data.get('language', 'en')
            
            result = self.evaluate_example(text, ground_truth, language)
            self.results.append(result)
        
        total_time = time.time() - self.start_time
        print(f"\n‚úÖ Benchmark complete!")
        print(f"   Total time: {total_time:.2f}s")
        print(f"   Average: {total_time/len(self.results):.3f}s per example")
        
        return self.results
    
    def save_results_csv(self):
        """
        Save results to CSV
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        csv_path = self.output_dir / f'benchmark_results_{timestamp}.csv'
        
        print(f"\n[5/5] Saving results to {csv_path}...")
        
        df_results = pd.DataFrame(self.results)
        df_results.to_csv(csv_path, index=False)
        
        print(f"‚úÖ Results saved: {len(self.results)} examples")
        
        return csv_path
    
    def calculate_metrics(self):
        """
        Calculate performance metrics
        """
        df = pd.DataFrame(self.results)
        
        # Basic metrics
        y_true = df['ground_truth'].values
        y_pred = df['detected'].values
        y_blocked = df['blocked'].values
        
        # Confusion matrix
        tp = np.sum((y_true == 1) & (y_pred == 1))
        tn = np.sum((y_true == 0) & (y_pred == 0))
        fp = np.sum((y_true == 0) & (y_pred == 1))
        fn = np.sum((y_true == 1) & (y_pred == 0))
        
        # Calculate metrics
        accuracy = (tp + tn) / len(y_true) if len(y_true) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        # Blocking metrics
        blocked_correct = np.sum((y_true == 1) & (y_blocked == 1)) + np.sum((y_true == 0) & (y_blocked == 0))
        blocking_accuracy = blocked_correct / len(y_true) if len(y_true) > 0 else 0
        
        # False positive/negative rates
        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
        
        metrics = {
            'total_examples': len(self.results),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'blocking_accuracy': blocking_accuracy,
            'true_positives': int(tp),
            'true_negatives': int(tn),
            'false_positives': int(fp),
            'false_negatives': int(fn),
            'false_positive_rate': fpr,
            'false_negative_rate': fnr,
            'avg_confidence': float(df['confidence'].mean()),
            'avg_processing_time_ms': float(df['processing_time_ms'].mean()),
            'total_time_seconds': time.time() - self.start_time if self.start_time else 0
        }
        
        return metrics


def main():
    """
    Main execution
    """
    print("\n" + "="*80)
    print("STARTING BENCHMARK EVALUATION")
    print("="*80)
    
    # Initialize evaluator
    evaluator = BenchmarkEvaluator()
    
    # Load dataset
    df = evaluator.load_dataset()
    if df is None:
        print("‚ùå Failed to load dataset. Exiting.")
        return
    
    # Augment dataset (optional)
    print("\n" + "="*80)
    print("DATASET AUGMENTATION")
    print("="*80)
    
    augment = input("\nAugment dataset with synthetic examples? (y/n): ").lower().strip()
    if augment == 'y':
        num_synthetic = int(input("How many synthetic examples? (default 100): ") or 100)
        df = evaluator.augment_dataset(df, num_synthetic)
    
    # Run benchmark
    print("\n" + "="*80)
    print("BENCHMARK EXECUTION")
    print("="*80)
    
    sample_size = input(f"\nSample size (press Enter for all {len(df)} examples): ").strip()
    sample_size = int(sample_size) if sample_size else None
    
    results = evaluator.run_benchmark(df, sample_size)
    
    # Save results
    csv_path = evaluator.save_results_csv()
    
    # Calculate metrics
    print("\n" + "="*80)
    print("PERFORMANCE METRICS")
    print("="*80)
    
    metrics = evaluator.calculate_metrics()
    
    print(f"\nüìä Overall Performance:")
    print(f"   Total Examples: {metrics['total_examples']}")
    print(f"   Accuracy: {metrics['accuracy']:.3f} ({metrics['accuracy']*100:.1f}%)")
    print(f"   Precision: {metrics['precision']:.3f}")
    print(f"   Recall: {metrics['recall']:.3f}")
    print(f"   F1 Score: {metrics['f1_score']:.3f}")
    print(f"   Blocking Accuracy: {metrics['blocking_accuracy']:.3f} ({metrics['blocking_accuracy']*100:.1f}%)")
    
    print(f"\nüìà Confusion Matrix:")
    print(f"   True Positives:  {metrics['true_positives']}")
    print(f"   True Negatives:  {metrics['true_negatives']}")
    print(f"   False Positives: {metrics['false_positives']}")
    print(f"   False Negatives: {metrics['false_negatives']}")
    
    print(f"\n‚ö° Performance:")
    print(f"   Avg Processing Time: {metrics['avg_processing_time_ms']:.2f}ms")
    print(f"   Total Time: {metrics['total_time_seconds']:.2f}s")
    print(f"   Throughput: {metrics['total_examples']/metrics['total_time_seconds']:.2f} examples/sec")
    
    print(f"\n‚úÖ Results saved to: {csv_path}")
    print(f"\nNext: Run 'python3 benchmark/generate_report.py {csv_path}' for detailed analysis")


if __name__ == '__main__':
    main()
